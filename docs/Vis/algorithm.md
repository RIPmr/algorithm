# Vis Algorithm

可视化算法笔记

## 数据降维方法
降维是一种对高维度特征数据预处理的方法。<br>
降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。

在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。
降维也成为应用非常广泛的数据预处理方法。

降维具有如下一些优点：
- 使得数据集更易使用
- 降低算法的计算开销
- 去除噪声
- 使得结果容易理解

降维的算法有很多，比如奇异值分解(SVD)、主成分分析(PCA)、因子分析(FA)、独立成分分析(ICA)等。

> 线性方法
### 主成分分析(Principal component analysis, PCA)
主成分分析（Principal Component Analysis，PCA），是一种统计方法，也是数学上用来降维的一种方法。<br>
主成分分析经常用于减少数据集的维数，同时保持数据集中的对方差贡献最大的特征，<br>
也就是通过正交变换将一组可能存在相关性的变量转换为一组<strong>线性不相关</strong>的变量，转换后的这组变量叫<strong>主成分</strong>。
<br><br>
例如，一个在三维空间完成的聚类问题，我们可以通过 PCA 将特征降低到二维平面进行可视化分析。
<center><img width="40%" src="Vis/algoPics/1.jpg"/></center>

#### PCA的提出
主成分分析由卡尔·皮尔逊于1901年发明，用于分析数据及建立数理模型。<br>
其方法主要是通过对协方差矩阵进行特征分解，以得出数据的<strong>主成分（特征向量）</strong>与它们的<strong>权值（特征值）</strong>。<br>
因为PCA仅保留了特征的主成分，所以PCA是一种<strong>有损的压缩方式</strong>。

#### 算法流程
求样本 xi 的 n' 维的主成分其实就是求样本集的协方差矩阵 XX^T/m 的前 n' 个特征值对应特征向量矩阵 P ，然后对于每个样本 xi ,做如下变换 yi=Pxi ，即达到降维的PCA目的。

具体的算法流程：

输入： n 维样本集 X = (x1, x2, ..., xm) ，要降维到的维数 n' .

输出：降维后的样本集 Y

1.对所有的样本进行中心化 xi = xi - Σ(j=1→m)xj/m

2.计算样本的协方差矩阵 C = XX^T/m

3.求出协方差矩阵的特征值及对应的特征向量

4.将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P

5.Y = PX 即为降维到k维后的数据

```注意：有时候，我们不指定降维后的 n' 的值，而是换种方式，指定一个降维到的主成分比重阈值 t∈(0, 1]。```

#### 降到多少维才合适？
从 PCA 的执行流程中，我们知道，需要为 PCA 指定目的维度 k 。<br>
如果降维不多，则性能提升不大；如果目标维度太小，则又丢失了许多信息。<br>
通常来讲取全部维度的前10%是足够的。

#### 不要提前优化
由于 PCA 减小了特征维度，因而也有可能带来过拟合的问题。<br>
PCA 不是必须的，在机器学习中，一定谨记不要提前优化，<br>
只有当算法运行效率不尽如如人意时，再考虑使用 PCA 或者其他特征降维手段来提升训练速度。

#### 总结
作为一个非监督学习的降维方法，它只需要特征值分解就可以对数据进行压缩，去噪，因此在实际场景应用很广泛。<br>

PCA算法的主要优点：
- 仅仅需要以方差衡量信息量，不受数据集以外的因素影响
- 各主成分之间正交，可消除原始数据成分间的相互影响的因素
- 计算方法简单，主要运算是特征值分解，易于实现

PCA算法的主要缺点：
- 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强
- 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响


为了克服PCA的一些缺点，出现了很多PCA的变种，比如为解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。

### 多维尺度分析(Multidimensional scaling, MDS)
多维尺度分析（Multidimensional scaling， MDS）也称做多维尺度变换，多维标度或多维尺度法等。

MDS是根据具有很多维度的样本或变量之间的相似性（距离近）或非相似性（距离远，即通过计算其距离）来对其进行分类的一种统计学研究方法。<br>
在MDS-map中，用空间（space）和距离（distance）来体现各个点之间的关系，来判断网络中各个点的分布情况，网络的密集情况等。

#### 算法流程
MDS方法有5个关键的要素，分别为主体、客体、准则、准则权重、主体权重。具体定义为：

1）客体：被评估的对象。可以认为是待分类的几种类别，数量M。

2）主体：评估客体的单位。就是训练数据。N个

3）准则：根据研究目的自行定义，用以评估客体优劣的标准。K个

4）准则权重：主体衡量准则重要性后，对每个准则分别赋予权重值。P个

5）主体权重：研究者权衡准则重要性后，对主体赋予权重值。N个

对于要分析的数据包括I个物体，定义一个距离函数的集合，其中δi,j是第i个和第j个对象之间的距离。于是有
<center><img width="30%" src="Vis/algoPics/2.jpg"/></center>

MDS算法的目的就是根据这个Δ，寻找I个向量x1, x2, ... xn ∈ R^N，使||xi - xj|| ≈ δi,j，对于i，j属于I。这里这个||.||是向量的范数，在经典的MDS，该规范是欧氏距离，但广义的讲，这个规范可以是任意函数。

也就是说，MDS试图找到一个子空间Rn，I个物体嵌入在这个子空间中，而彼此的相似度被尽可能的保留。如果这个子空间的维数N选择为2或者3，可以画出向量xj获得一个I个物体相似性的一个可视化的结果。
```注意向量xj不是唯一的：对于欧式距离，可以被任意旋转和变换，因为这些变换不会改变样本间的距离。```

有很多途径可以得到向量xj。通常MDS可以被看做是一个优化问题，寻找（x1，...xI）被看成是最小化目标函数，例如
<center><img width="30%" src="Vis/algoPics/3.jpg"/></center>


可以利用一些数值优化的方法得到这个最优解。

#### 总结
MDS算法的主要优点：
- 研究者可以利用得到的位置结构图将研究对象进行分类
- 可以对隐藏在数据背后的空间维度做出相应的判断和解释
- MDS通过把所研究对象的数量关系转化为直观图形，达到直观展现研究对象的目的

MDS算法的主要缺点：
- 分析结果不是唯一的
- 结果可以在空间中旋转和平移，这为分析者对结果的解释制造了难度

> 非线性方法
### t分布随即近邻嵌入(t-Distributed Stochastic Neighbor Embedding, t-SNE)
#### t-SNE的提出
t-SNE(t-distributed stochastic neighbor embedding)是用于降维的一种机器学习算法，由 Laurens van der Maaten 和 Geoffrey Hinton在08年提出。t-SNE 作为一种非线性降维算法，常用于流行学习(manifold learning)的降维过程中并与LLE进行类比，非常适用于高维数据降维到2维或者3维，便于进行可视化。

t-SNE是由SNE(Stochastic Neighbor Embedding, SNE[Hinton and Roweis, 2002])发展而来。首先介绍SNE的基本原理，之后再扩展到t-SNE。最后是t-SNE的实现以及一些优化。

#### t-SNE的原理
t-SNE为高维特征空间在二维平面（或三维超平面，不过基本上总是使用二维空间）上寻找一个投影，使得在原本的n维空间中相距很远的数据点在屏幕上同样相距较远，而原本相近的点在平面上仍然相近。

本质上，近邻嵌入寻找保留了样本的邻居关系的新的维度较低的数据表示。


#### SNE的算法步骤
SNE是通过仿射(affinitie)变换将数据点映射到概率分布上，主要包括两个步骤：<br>
a) SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。<br>
b) SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。

SNE是先将欧几里得距离转换为条件概率来表达点与点之间的相似度，会倾向于保留数据中的局部特征。

#### Crowding问题

拥挤问题就是说各个簇聚集在一起，无法区分。<br>
比如有一高维度数据在降维到10维下，可以有很好的表达，但是降维到2维后无法得到可信映射。
<center><img width="100%" src="Vis/algoPics/4.jpg"/></center>
<center>随着维度的增大，大部分数据点都聚集在m维球的表面附近</center>

#### 改进的t-SNE
尽管SNE提供了很好的可视化方法，但是他很难优化，而且存在”crowding problem”(拥挤问题)。后续中，Hinton等人又提出了t-SNE的方法。与SNE不同，主要如下:

- 使用对称版的SNE，简化梯度公式
- 低维空间下，使用t分布替代高斯分布表达两点之间的相似度

t-SNE在低维空间下使用更重长尾分布的t分布来避免crowding问题和优化问题。也就是说，在<strong>高维</strong>空间下，我们使用<strong>高斯分布</strong>将距离转换为概率分布；在<strong>低维</strong>空间下，我们使用<strong>更加偏重长尾分布</strong>的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。
<center><img width="80%" src="Vis/algoPics/5.jpg"/></center>
<center>高斯分布和t分布</center>

#### 总结
t-sne的有效性，可以从下图中看到：<br>
横轴表示距离，纵轴表示相似度。对于较大相似度的点，t分布在低维空间中的距离需要稍小一点；而对于低相似度的点，t分布在低维空间中的距离需要更远。<br>
这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。
<center><img width="80%" src="Vis/algoPics/6.jpg"/></center>

t-SNE的梯度更新的优点：
- 对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来
- 这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远

t-SNE的缺点：
- 主要用于可视化，很难用于其他目的。比如测试集合降维，因为他没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度
- t-SNE倾向于保存局部特征，对于本征维数(intrinsic dimensionality)本身就很高的数据集，不可能完整的映射到2-3维的空间
- t-SNE没有唯一最优解，且没有预估部分。如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-sne中距离本身是没有意义，都是概率分布问题
- 训练太慢。有很多基于树的算法在t-sne上做一些改进

### 自组织映射(Self-organizing map, SOM)
自组织映射（SOM）网络，也被称为 Kohonen 网络或者胜者独占单元（WTU），是受人脑特征启发而提出的一种非常特殊的神经网络。

<center><img width="60%" src="Vis/algoPics/7.png"/></center>

与其他神经网络不同，SOM 神经元之间并不是通过权重相互连接的，相反，它们能够影响彼此的学习。<br>
SOM基于竞争性学习，其中输出神经元之间竞争激活，结果是在任意时间只有一个神经元被激活。这个激活的神经元被称为胜者神经元（winner-takes-all neuron）。<br>
这种竞争可以通过在神经元之间具有横向抑制连接（负反馈路径）来实现，其结果是神经元被迫对自身进行重新组合。

SOM 通过竞争机制进行学习，可以认为它是 PCA 的非线性推广，因此 SOM 可以像 PCA 一样用于降维。

#### 自组织的过程
自组织的过程包括以下四个主要方面：

- 初始化：所有连接权重都用小的随机值进行初始化。
- 竞争：对于每种输入模式，神经元计算它们各自的判别函数值，为竞争提供基础。具有最小判别函数值的特定神经元被宣布为胜利者。
- 合作：获胜的神经元决定了兴奋神经元拓扑邻域的空间位置，从而为相邻神经元之间的合作提供了基础。
- 适应：受激神经元通过适当调整相关的连接权重，减少与输入模式相关的判别函数值，使得获胜的神经元对相似输入模式的后续应用的响应增强。

#### 竞争过程
如果输入空间是D维（即有D个输入单元），我们可以把输入模式写成x = {xi:i = 1,...,D}，输入单元i和神经元j之间在计算层的连接权重可以写成wj={wji:j = 1,...,N; i = 1,...,D}，其中N是神经元的总数。

然后，我们可以将我们的判别函数定义为输入向量x和每个神经元j的权向量wj之间的平方欧几里德距离。

<center><img width="20%" src="Vis/algoPics/8.jpg"/></center>

换句话说，权重向量最接近输入向量（即与其最相似）的神经元被宣告为胜利者。这样，连续的输入空间可以通过神经元之间的一个简单的竞争过程被映射到神经元的离散输出空间。

#### 合作过程
在神经生物学研究中，我们发现在一组兴奋神经元内存在横向的相互作用。当一个神经元被激活时，最近的邻居节点往往比那些远离的邻居节点更兴奋。并且存在一个随距离衰减的拓扑邻域。

我们想为我们的SOM中的神经元定义一个类似的拓扑邻域。 如果Sij是神经元网格上神经元i和j之间的横向距离，我们取

<center><img width="20%" src="Vis/algoPics/9.jpg"/></center>

作为我们的拓扑邻域，其中I(x)是获胜神经元的索引。该函数有几个重要的特性：它在获胜的神经元中是最大的，且关于该神经元对称，当距离达到无穷大时，它单调地衰减到零，它是平移不变的（即不依赖于获胜的神经元的位置）。

SOM的一个特点是σ需要随着时间的推移而减少。常见的时间依赖性关系是指数型衰减：

<center><img width="20%" src="Vis/algoPics/12.jpg"/></center>

#### 适应过程
显然，我们的SOM必须涉及某种自适应或学习过程，通过这个过程，输出节点自组织，形成输入和输出之间的特征映射。

地形邻域的一点是，不仅获胜的神经元能够得到权重更新，它的邻居也将更新它们的权重，尽管不如获胜神经元更新的幅度大。在实践中，适当的权重更新方式是

<center><img width="30%" src="Vis/algoPics/10.jpg"/></center>

其中我们有一个依赖于时间的学习率
<center><img width="20%" src="Vis/algoPics/11.jpg"/></center>

该更新适用于在多轮迭代中的所有训练模式x。

每个学习权重更新的效果是将获胜的神经元及其邻居的权向量wi向输入向量x移动。对该过程的迭代进行会使得网络的拓扑有序。

#### 排序和收敛
如果正确选择参数（σ0,τσ,η0,τη），我们可以从完全无序的初始状态开始，并且SOM算法将逐步使得从输入空间得到的激活模式表示有序化。（但是，可能最终处于特征映射具有拓扑缺陷的亚稳态。）

这个自适应过程有两个显著的阶段：

- 排序或自组织阶段：在这期间，权重向量进行拓扑排序。通常这将需要多达1000次的SOM算法迭代，并且需要仔细考虑邻域和学习速率参数的选择。

- 收敛阶段：在此期间特征映射被微调（fine tune），并提供输入空间的精确统计量化。通常这个阶段的迭代次数至少是网络中神经元数量的500倍，而且参数必须仔细选择。

#### 可视化自组织过程
假设我们在连续的二维输入空间中有四个数据点（×），并且希望将其映射到离散一维输出空间中的四个点上。输出节点映射到输入空间中的点（∘）。随机初始化权重使得∘的起始位置落在随机落在输入空间的中心。
<center><img width="40%" src="Vis/algoPics/13.png"/></center>

我们随机选择一个数据点（⊗）进行训练。最接近的输出点表示获胜的神经元（⧫）。获胜的神经元向数据点移动一定量，并且两个相邻的神经元以较小的量移动（箭头指示方向）。
<center><img width="40%" src="Vis/algoPics/14.png"/></center>

接下来，我们随机选择另一个数据点进行训练（⊗）。最接近的输出点给出新的获胜神经元（⧫）。获胜的神经元向数据点移动一定量，并且一个相邻的神经元也朝该数据点移动较小的量（箭头指示方向）。
<center><img width="40%" src="Vis/algoPics/15.png"/></center>

我们随机挑选数据点进行训练（⊗）。每个获胜的神经元向数据点移动一定的量，其相邻的神经元以较小的量向数据点移动（箭头指示方向）。最终整个输出网格将自身重新组织以表征输入空间。
<center><img width="40%" src="Vis/algoPics/16.png"/></center>

维基百科上给出的一个SOM学习过程示意图：
<center><img width="80%" src="Vis/algoPics/17.png"/></center>
一个自组织映射训练的例证。蓝色斑点是训练数据的分布，而小白色斑点是从该分布中抽取得到的当前训练数据。首先（左图）SOM节点被任意地定位在数据空间中。我们选择最接近训练数据的节点作为获胜节点（用黄色突出显示）。它被移向训练数据，包括（在较小的范围内）其网格上的相邻节点。经过多次迭代后，网格趋于接近数据分布（右图）。

#### 算法步骤
- 初始化 - 为初始权向量wj选择随机值
- 采样 - 从输入空间中抽取一个训练输入向量样本x
- 匹配 - 找到权重向量最接近输入向量的获胜神经元I(x)
- 更新 - 更新权重向量 <img width="30%" src="Vis/algoPics/10.jpg"/>
- 继续 - 继续回到步骤2，直到特征映射趋于稳定

#### 总结
有一个空间连续的输入空间，其中包含我们的输入向量。我们的目的是将其映射到低维的离散输出空间，其拓扑结构是通过在网格中布置一系列神经元形成的。SOM算法提供了称为特征映射的非线性变换。


### 等距特征映射(lsometric Feature Mapping, ISOMAP)
#### 流形(Manifold)
流形(Manifold)是局部具有欧式空间性质的空间。黎曼流形就是以光滑的方式在每一点的切空间上指定了欧式内积的微分流形。

#### Isomap的思路
<center><img width="30%" src="Vis/algoPics/18.png"/><img width="30%" src="Vis/algoPics/19.png"/><img width="30%" src="Vis/algoPics/20.png"/></center>

图A中一个点是一个样本，样本是三个维度的，而图中的各点组成的曲面是一个流形。传统的PCA、LDA都不能处理这个非线性的降维问题。但由于这是个流形，如果能将该曲面以某种方式展开成为平面，那么就完成了从3维到2维的降维。后面就可以用上述MDS的方式继续做降维。

```至此，该问题转化为“如何把曲面展开平面”（即“如何把流形展开”）。```

以宏观视角来看，这些样本是3维的无疑；但如果以微观视角来看，由于流形各处都是“光滑”的，所以只要足够小，那么就近似于平面。为了把局部微观的平面扩展到全局宏观的平面，就需要打断遥远点的联系，只保留临近点的联系。

引入图论框架，连接相邻的点构建一个连接图，被称作“邻接图”（neighborhood graph）

连接相邻的点的具体方式有二：
- 一是最近的k个点
- 二是以r为半径划定一个区域（多维的），内部的就算“临近的”

上图C就是<strong>三维欧式空间</strong>里的<strong>二维流形</strong>在<strong>二维欧式空间</strong>的<strong>近似</strong>对应。蓝线是流形真实展开后的距离；红线是用临近点的方式近似展开后的距离。二者不能完全重合，所以叫“近似”。


## 数据聚类方法
<center><img width="80%" src="Vis/algoPics/22.jpg"/></center>
聚类(Clustering)是按照某个特定标准(如距离)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。也即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。

### 聚类的一般过程
- 数据准备：特征标准化和降维
- 特征选择：从最初的特征中选择最有效的特征，并将其存储在向量中
- 特征提取：通过对选择的特征进行转换形成新的突出特征
- 聚类：基于某种距离函数进行相似度度量，获取簇
- 聚类结果评估：分析聚类结果，如距离误差和(SSE)等

<center><img width="90%" src="Vis/algoPics/21.jpg"/></center>
Minkowski距离就是Lp范数（p≥1)，而 Manhattan 距离、Euclidean距离、Chebyshev距离分别对应 p = 1,2,∞时的情形。

### K均值(K-means)
<center><img width="60%" src="Vis/algoPics/k-means.gif"/></center>

#### 算法流程
```
创建k个点作为初始质心(通常是随机选择)
当任意一个点的簇分配结果发生改变时
	对数据集中的每个数据点
		对每个质心
			计算质心与数据点之间的距离
		将数据点分配到距其最近的簇
	对每个簇，计算簇中所有点的均值并将均值作为质心
```

#### k-means的特点
- 需要提前确定k值
- 对初始质心点敏感
- 对异常数据敏感

#### 交互式k-means demo
<iframe src="Vis/d3demo/Visualizing K-Means Clustering.html" scrolling="no" frameborder="0" height="550" width="100%"></iframe>

### 均值漂移算法(Mean Shift)
<center><img width="40%" src="Vis/algoPics/meanShift.gif"/><img width="40%" src="Vis/algoPics/meanShiftMulti.gif"/></center>
均值漂移聚类是基于滑动窗口的算法，它试图找到数据点的密集区域。这是一个基于质心的算法，这意味着它的目标是定位每个组/类的中心点，通过将中心点的候选点更新为滑动窗口内点的均值来完成。然后，在后处理阶段对这些候选窗口进行过滤以消除近似重复，形成最终的中心点集及其相应的组。

#### 算法流程
```
将考虑二维空间中的一组点，从一个以 C 点（随机选择）为中心，以半径 r 为核心的圆形滑动窗口开始。
均值漂移是一种爬山算法，它包括在每一步中迭代地向更高密度区域移动，直到收敛。

在每次迭代中，滑动窗口通过将中心点移向窗口内点的均值（因此而得名）来移向更高密度区域。
滑动窗口内的密度与其内部点的数量成正比。
自然地，通过向窗口内点的均值移动，它会逐渐移向点密度更高的区域。

继续按照均值移动滑动窗口直到没有方向在核内可以容纳更多的点。
一直移动这个圆直到密度（即窗口中的点数）不再增加。

步骤 1 到 3 的过程是通过许多滑动窗口完成的，直到所有的点位于一个窗口内。
当多个滑动窗口重叠时，保留包含最多点的窗口。然后根据数据点所在的滑动窗口进行聚类。
```

#### 特点
- 与 K-means 聚类相比，这种方法不需要选择簇数量，因为均值漂移自动发现这一点。这是一个巨大的优势。
- 聚类中心朝最大点密度聚集的事实也是非常令人满意的，因为理解和适应自然数据驱动的意义是非常直观的。
- 它的缺点是窗口大小/半径「r」的选择可能是不重要的。

### 高斯混合模型(Gaussian Mixture Model)
<center><img width="50%" src="Vis/algoPics/GMMs.gif"/></center>
高斯混合模型(GMM)假设存在一定数量的高斯分布，每个分布代表一个簇。

因此，高斯混合模型倾向于将属于单一分布的数据点聚在一起。

#### 算法步骤
```
1. 选择簇的数量（与K-Means类似）并随机初始化每个簇的高斯分布参数（均值和方差）。也可以先观察数据给出一个相对精确的均值和方差。 
2. 给定每个簇的高斯分布，计算每个数据点属于每个簇的概率。一个点越靠近高斯分布的中心就越可能属于该簇。 
3. 基于这些概率我们计算高斯分布参数使得数据点的概率最大化，可以使用数据点概率的加权来计算这些新的参数，权重就是数据点属于该簇的概率。 
4. 重复迭代2和3直到在迭代中的变化不大。 
```

#### 特点
- GMMs使用均值和标准差，簇可以呈现出椭圆形而不是仅仅限制于圆形。
- K-Means是GMMs的一个特殊情况，是方差在所有维度上都接近于0时簇就会呈现出圆形。 
- GMMs是使用概率，所有一个数据点可以属于多个簇。
例如数据点X可以有百分之20的概率属于A簇，百分之80的概率属于B簇。也就是说GMMs可以支持混合。

### DBSCAN算法(Density-Based Spatial Clustering of Applications with Noise)
DBSCAN 是一种基于密度的聚类算法，它类似于均值漂移，但具有一些显著的优点。直观效果上看，DBSCAN算法可以找到样本点的全部密集区域，并把这些密集区域当做一个一个的聚类簇。

<center><img width="50%" src="Vis/algoPics/DBSCAN.webp"/></center>

4种点的关系：密度直达，密度可达，密度相连，非密度相连
<center><img width="70%" src="Vis/algoPics/23.jpg"/></center>

如果P为核心点，Q在P的R邻域内，那么称P到Q密度直达。任何核心点到其自身密度直达，密度直达不具有对称性，如果P到Q密度直达，那么Q到P不一定密度直达。

如果存在核心点P2，P3，……，Pn，且P1到P2密度直达，P2到P3密度直达，……，P(n-1)到Pn密度直达，Pn到Q密度直达，则P1到Q密度可达。密度可达也不具有对称性。

如果存在核心点S，使得S到P和Q都密度可达，则P和Q密度相连。密度相连具有对称性，如果P和Q密度相连，那么Q和P也一定密度相连。密度相连的两个点属于同一个聚类簇。

如果两个点不属于密度相连关系，则两个点非密度相连。非密度相连的两个点属于不同的聚类簇，或者其中存在噪声点。

#### 算法流程
```
1.标记所有对象为unvisited
2.当有标记对象时
	1.随机选取一个unvisited对象p
	2.标记p为visited
	3.如果p的e邻域内至少有M个对象，则
		1.创建一个新的簇C,并把p放入C中
		2.设N是p的e邻域内的集合，对N中的每个点p'
			1.如果点p'是unvisited
				1.标记p' 为visited
				2.如果p'的e邻域至少有M个对象，则把这些点添加到N
				3.如果p'还不是任何簇的成员，则把p' 添加到C
	3.保存C
4.否则标记p为噪声
```
```构建ε邻域的过程可以使用kd-tree进行优化```

#### 特点
- 基于密度，对远离密度核心的噪声点鲁棒
- 无需知道聚类簇的数量
- 可以发现任意形状的聚类簇

DBSCAN 不需要固定数量的簇；它会将异常值识别为噪声，而不像均值漂移，即使数据点非常异常，也会简单地将它们分入簇中。另外，它能够很好地找到任意大小和任意形状的簇。

DBSCAN 的主要缺点是当簇的密度不同时，它的表现不如其他聚类算法。这是因为当密度变化时，用于识别邻域点的距离阈值 ε 和 minPoints 的设置将会随着簇而变化。这个缺点也会在非常高维度的数据中出现，因为距离阈值 ε 再次变得难以估计。

#### 交互式DBSCAN demo
<iframe src="Vis/d3demo/Visualizing DBSCAN Clustering.html" scrolling="no" frameborder="0" height="550" width="100%"></iframe>

### 层次聚类(Hierarchical clustering)
<center><img width="70%" src="Vis/algoPics/hierarchical.gif"/></center>

前面介绍的几种算法确实可以在较小的复杂度内获取较好的结果，但是这几种算法却存在一个链式效应的现象，比如：A与B相似，B与C相似，那么在聚类的时候便会将A、B、C聚合到一起，但是如果A与C不相似，就会造成聚类误差，严重的时候这个误差可以一直传递下去。

为了降低链式效应，这时候层次聚类就能够发挥作用了。

#### 算法步骤
层次聚类算法 (hierarchical clustering) 将数据集划分为一层一层的 clusters，后面一层生成的 clusters 基于前面一层的结果。层次聚类算法一般分为两类：
- Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 cluster，每次按一定的准则将最相近的两个 cluster 合并生成一个新的 cluster，如此往复，直至最终所有的对象都属于一个 cluster。这里主要关注此类算法。
- Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 cluster，每次按一定的准则将某个 cluster 划分为多个 cluster，如此往复，直至每个对象均是一个 cluster。

#### 特点
层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。

### 谱聚类(Spectral clustering)
谱聚类是一种基于图论的聚类方法，通过对样本数据的拉普拉斯矩阵的特征向量进行聚类，从而达到对样本数据聚类的母的。谱聚类可以理解为将高维空间的数据映射到低维，然后在低维空间用其它聚类算法（如K-Means）进行聚类。

#### 谱
方阵作为线性算子，它的所有特征值的全体统称为方阵的谱。方阵的谱半径为最大的特征值。矩阵A的谱半径是矩阵A^TA的最大特征值。

#### 算法流程
```
1) 确定图上节点关系度量，得到相似性度量矩阵
2）根据相似性度量矩阵得到拉普拉斯矩阵
3）对拉普拉斯矩阵求解前K KK个最小特征值对应的特征向量，即为节点的向量表示
4）采用聚类算法对节点向量进行聚类
```

#### 特点
- 相似性度量矩阵限制了数据的表示为 m^2
- 谱聚类对相似性度量矩阵的向量表示存在损失
- 谱聚类的向量表示数学形式美观，代码实现方便
- 聚类的效果与相似性度量矩阵的计算、表示，以及最终采用的聚类算法有关