# AdvMath

高等数学

## 方向导数
顾名思义，方向导数就是某个方向上的导数：

<table><tr>
	<td>
		<center><img width="60%" src="Pics/math/1.png"/></center>
		<center>方向</center>
	</td>
	<td>
		<center><img width="60%" src="Pics/math/2.png"/></center>
		<center>函数f(x,y)在这个方向上的图像</center>
	</td>
</tr></table>

我们知道一元函数中导数就是该点切线斜率；

函数f(x,y)的A点在这个方向上也是有切线的，其切线的斜率就是方向导数：

<table><tr>
	<td>
		<center><img width="60%" src="Pics/math/3.png"/></center>
		<center>一元导数（全导数）</center>
	</td>
	<td>
		<center><img width="70%" src="Pics/math/4.png"/></center>
		<center>方向导数（偏导数）</center>
	</td>
</tr></table>

## 梯度
很显然，A点不止一个方向，而是360°都有方向，且每个方向都有方向导数：

<table><tr>
	<td>
		<center><img width="50%" src="Pics/math/5.png"/></center>
	</td>
	<td>
		<center><img width="80%" src="Pics/math/6.png"/></center>
	</td>
</tr></table>

这就引出了梯度的定义：

> **梯度:** 是一个矢量，其方向上的方向导数最大，其大小正好是此最大方向导数.

严格的数学定义：

<div align=center><img width="50%" src="Pics/math/7.png"/></div>

具有一阶连续偏导数，意味着可微。可微意味着函数f(x,y)在各个方向的切线都在同一个平面上，也就是切平面：

<div align=center><img width="40%" src="Pics/math/8.png"/></div>

实际应用中我们通常需要去寻找所有方向导数中梯度的最大值 (所有方向导数中会存在并且只存在一个最大值)，这个最大值的方向我们就取名为梯度方向，那么这个最大值在哪个方向取得？值是多少？

求解这个问题的过程可以这样形象比喻：要在起伏不平的山谷中找到最低点：

<div align=center><img width="60%" src="Pics/math/9.png"/></div>

下面列举几种常见的最优化方法：

### 梯度下降法 (Gradient Descent)
梯度下降法是最早最简单，也是最为常用的最优化方法。

梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局最优解。

<strong>一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。</strong>

梯度下降法的优化思想是<strong>用当前位置负梯度方向作为搜索方向</strong>，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。

<table><tr>
	<td>
		<center><img width="70%" src="Pics/math/10.png"/></center>
		<center>沿负梯度方向搜索下降</center>
	</td>
	<td>
		<center><img width="90%" src="Pics/math/11.png"/></center>
		<center>“之字形”下降</center>
	</td>
</tr></table>

!><strong>缺陷：</strong><br>· 靠近极小值时收敛速度减慢<br>· 可能会“之字形”地下降，路径不平滑<br>· 不一定找到全局最优解

在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法：

### 批量梯度下降法（Batch Gradient Descent）
所谓批量梯度下降就是当求解多元函数最优化问题时，对每一个自变量求偏导，逐步迭代更新。

!> <strong>梯度下降中的α</strong><br>
α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往非常重要，α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！

<div align=center><img width="50%" src="Pics/math/15.png"/></div>
<center>α过大或过小</center>

!><strong>缺陷：</strong><br>· 它得到的是一个全局最优解，但每一步迭代都要用到训练集所有的数据，迭代速度相当慢<br>· 样本个数m，n维向量下，一次迭代需要把m个样本全部带入计算，迭代一次计算量为m*pow(n,2)

因此，引入了另一种优化方法——随机梯度下降：

### 随机梯度下降（Stochastic Gradient Descent）
批量梯度下降是计算一个维度中所有的数据,取平均来当做每一次梯度下降的step。

这样做虽然准确,但是<strong>每次要计算一个维度的所有数据的梯度,花费资源较大</strong>。

所以才有了随机梯度下降思想:

<strong>每次只随机取一个维度中的一条数据求梯度,来当做这个维度梯度下降的step</strong>。

<table><tr>
	<td>
		<center><img width="70%" src="Pics/math/12.png"/></center>
		<center>批量梯度下降</center>
	</td>
	<td>
		<center><img width="50%" src="Pics/math/13.png"/></center>
		<center>随机梯度下降</center>
	</td>
</tr></table>

!><strong>缺陷：</strong><br>· 噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向<br>· 最终的结果往往是在全局最优解附近，适用于大规模训练样本情况

#### 两种方法的区别：
- BGD总是综合所有数据的梯度,取到的下降至一直很平滑,SGD随机抽取一条数据作为参数,步子很不稳定；
- 两算法最终都可以到达函数的最优解位置，虽然看起来SGD比BGD的误差要大一些,但是SGD随着迭代次数的增加,误差会越来越小；
- SGD因为每次只随机抽取一条数据来做梯度下降,计算代价比SGD小非常多，因此无论机器学习还是深度学习,对SGD的应用都非常广泛；
- 随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。

### 牛顿-拉弗森法（Newton-Raphson Method）
又称牛顿迭代法（Newton's method）。牛顿法是一种在实数域和复数域上近似求解方程的方法，方法使用函数 f(x) 的泰勒级数的前面几项来寻找方程 f(x) = 0 的根。<strong>牛顿法最大的特点就在于它的收敛速度很快。</strong>

#### 步骤
1. 首先，选择一个接近函数 f(x)零点的 x0，计算相应的 f(x0) 和切线斜率f'(x0)（这里f'表示函数 f 的导数）。然后我们计算穿过点(x0, f(x0)) 并且斜率为f'(x0)的直线和 x 轴的交点的x坐标，也就是求如下方程的解：

<div align=center><img width="35%" src="Pics/math/16.png"/></div>

2. 将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f(x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示：

<div align=center><img width="20%" src="Pics/math/17.png"/></div>

3. 已经证明，如果f'是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值x0位于这个邻近区域内，那么牛顿法必定收敛。并且，如果f'(x)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示：

<div align=center><img width="60%" src="Pics/math/18.gif"/></div>

#### 换个视角
从几何上说，牛顿法就是用一个二次曲面去拟合当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

<div align=center><img width="20%" src="Pics/math/14.png"/></div>
<center>红色:牛顿法的迭代路径，绿色:梯度下降法的迭代路径</center>

#### 优缺点

> 优点：二阶收敛，收敛速度快；

!> 缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

#### 牛顿法和梯度下降法的效率对比：
- 从本质上看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。
- 如果更通俗地说，比如想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）

### 拟牛顿法 (Quasi-Newton Methods)
拟牛顿法是求解非线性优化问题最有效的方法之一，<strong>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</strong>

拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

#### 步骤
1. 首先构造目标函数在当前迭代xk的二次模型：

<div align=center><img width="40%" src="Pics/math/18.png"/></div>

2. 这里Bk是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：

<div align=center><img width="25%" src="Pics/math/19.png"/></div>

3. 其中我们要求步长 ak 满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hessian矩阵 Bk 代替真实的Hessian矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵 Bk 的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型：

<div align=center><img width="50%" src="Pics/math/20.png"/></div>

4. 我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求：

<div align=center><img width="40%" src="Pics/math/21.png"/></div>

5. 从而得到：

<div align=center><img width="40%" src="Pics/math/22.png"/></div>

这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。

### 共轭梯度法 (Fletcher-Reeves Method)
共轭梯度法是介于最速下降法与牛顿法之间的一个方法，<strong>它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点；</strong>

共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。

<strong>其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</strong>

<div align=center><img width="20%" src="Pics/math/23.png"/></div>
<center>绿色为梯度下降法，红色代表共轭梯度法</center>

## 启发式方法
启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,<strong>利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。</strong>

### 退火算法(Simulated Annealing)
模拟退火算法来源于固体退火原理，是一种基于概率的算法，将固体加温至充分高，再让其徐徐冷却，加温时，固体内部粒子随温升变为无序状，内能增大，而徐徐冷却时粒子渐趋有序，在每个温度都达到平衡态，最后在常温时达到基态，内能减为最小。

模拟退火算法以一定的概率来接受一个比当前解要差的解,因此有可能会跳出这个局部的最优解,达到全局的最优解。

### 遗传算法(Genetic Algorithm)
模拟达尔文生物进化论的自然选择和遗传学机理的生物进化过程的计算模型，是一种通过模拟自然进化过程搜索最优解的方法。

遗传算法是从代表问题可能潜在的解集的一个<strong>种群（population）</strong>开始的，而一个种群则由经过<strong>基因（gene）</strong>编码的一定数目的个体(individual)组成。

每个个体实际上是<strong>染色体(chromosome)</strong>带有特征的实体。

由于仿照基因编码的工作很复杂，我们往往进行简化，如二进制编码，初代种群产生之后，按照适者生存和优胜劣汰的原理，<strong>逐代（generation）演化</strong>产生出越来越好的近似解；

在每一代，根据问题域中个体的<strong>适应度（fitness）</strong>大小<strong>选择（selection）</strong>个体，并借助于自然遗传学的<strong>遗传算子（genetic operators）</strong>进行<strong>组合交叉（crossover）</strong>和<strong>变异（mutation）</strong>，产生出代表新的解集的种群。

这个过程将导致种群像自然进化一样的后生代种群比前代更加适应于环境，末代种群中的最优个体经过<strong>解码（decoding）</strong>，可以作为问题近似最优解。

### 差分进化算法(Differential Evolution)
DE 算法主要用于求解连续变量的全局优化问题，其主要工作步骤与其他进化算法基本一致，主要包括<strong>变异(Mutation)、交叉(Crossover)、选择(Selection)</strong>三种操作。

算法的基本思想是从某一随机产生的初始群体开始，利用从种群中随机选取的两个个体的差向量作为第三个个体的随机变化源，将差向量加权后按照一定的规则与第三个个体求和而产生变异个体，该操作称为<strong>变异</strong>。

然后，变异个体与某个预先决定的目标个体进行参数混合，生成试验个体，这一过程称之为<strong>交叉</strong>。

如果试验个体的适应度值优于目标个体的适应度值，则在下一代中试验个体取代目标个体，否则目标个体仍保存下来，该操作称为<strong>选择</strong>。

在每一代的进化过程中，每一个体矢量作为目标个体一次，算法通过不断地迭代计算，保留优良个体，淘汰劣质个体，引导搜索过程向全局最优解逼近。

#### DE与GA的不同之处
遗传算法是根据适应度值来控制父代杂交，变异后产生的子代被选择的概率值，在最大化问题中适应值大的个体被选择的概率相应也会大一些。

而差分进化算法变异向量是由父代差分向量生成，并与父代个体向量交叉生成新个体向量，直接与其父代个体进行选择。显然差分进化算法相对遗传算法的逼近效果更加显著。

DE多用于多维最优化问题，可以把DE看成是一种“改进的GA算法”。

#### DE的缺点
1. 搜索停滞

	种群个体较少，且生成新一代个体的适应值比原种群个体适应值差，导致个体难以更新，没有收敛到极值点。

2. 早熟收敛

	参数设置不当，收敛过快，局部最优问题。

### 蚁群算法(Ant System)
由意大利学者Dorigo、Maniezzo等人于20世纪90年代首先提出。他们在研究蚂蚁觅食的过程中，发现单个蚂蚁的行为比较简单，但是蚁群整体却可以体现一些智能的行为。例如蚁群可以在不同的环境下，寻找最短到达食物源的路径。这是因为蚁群内的蚂蚁可以通过某种信息机制实现信息的传递。后又经进一步研究发现，蚂蚁会在其经过的路径上释放一种可以称之为“信息素”的物质，蚁群内的蚂蚁对“信息素”具有感知能力，它们会沿着“信息素”浓度较高路径行走，而每只路过的蚂蚁都会在路上留下“信息素”，这就形成一种类似正反馈的机制，这样经过一段时间后，整个蚁群就会沿着最短路径到达食物源了。

基本思路:
用蚂蚁的行走路径表示待优化问题的可行解，整个蚂蚁群体的所有路径构成待优化问题的解空间。路径较短的蚂蚁释放的信息素量较多，随着时间的推进，较短的路径上累积的信息素浓度逐渐增高，选择该路径的蚂蚁个数也愈来愈多。最终，整个蚂蚁会在正反馈的作用下集中到最佳的路径上，此时对应的便是待优化问题的最优解。


### 粒子群算法(Particle Swarm Optimization)
最早是由Eberhart和Kennedy于1995年提出，它的基本概念源于对鸟群觅食行为的研究。它的基本核心是利用群体中的个体对信息的共享从而使整个群体的运动在问题求解空间中产生从无序到有序的演化过程，从而获得问题的最优解。

在PSO中，每个优化问题的解都是搜索空间中的一只鸟，称之为"粒子"，而问题的最优解就对应于鸟群中寻找的"玉米地"。所有的粒子都具有一个位置向量（粒子在解空间的位置）和速度向量（决定下次飞行的方向和速度），并可以根据目标函数来计算当前的所在位置的适应值（fitness value）,可以将其理解为距离"玉米地"的距离。

在每次的迭代中，种群中的例子除了根据自身的经验（历史位置）进行学习以外，还可以根据种群中最优粒子的"经验"来学习，从而确定下一次迭代时需要如何调整和改变飞行的方向和速度。就这样逐步迭代，最终整个种群的例子就会逐步趋于最优解。

### 对比
#### 性能对比
遗传算法，粒子群算法，差分进化算法都属于进化算法的分枝，很多学者对这些算法进行了研究，通过不断的改进，提高了算法的性能，扩大了应用领域因此很有必要讨论这些算法的特点，针对不同应用领域和算法的适应能力，推荐不同的算法供使用将是十分有意义的工作．在文献中，作者针对广泛使用的 34 个基准函数分别对 DE，EA，PSO 进行了系列实验分析，对各种算法求解最优解问题进行了讨论.通过实验分析，DE 算法获得了最优性能，而且算法比较稳定，反复运算都能收敛到同一个解；PSO 算法收敛速度次之，但是算法不稳定，最终收敛结果容易受参数大小和初始种群的影响；EA 算法收敛速度相对比较慢，但在处理噪声问题方面，EA 能够很好的解决而 DE 算法很难处理这种噪声问题。

#### 指标对比
1. 编码标准     

	GA 采用二进制编码，PSO、DE 都采用浮点实数编码，近年来许多学者通过整数编码将GA 算法、PSO 算法应用与求解离散型问题，特别是 0-1 非线性优化为题，整数规划问题、混合整数规划问题，而离散的 DE 算法则研究的比较少，而采用混合编码技术的 DE 算法则研究更少。

2. 参数设置问题    

	DE 算法主要有三个参数（种群大小NP、缩放因子F、交叉概率CR）要调整，而且参数设置对结果影响不太明显，因此更容易使用。相对于 GA 和 PSO 算法的参数过多，不同的参数设置对最终结果影响也比较大，因此在实际使用中，要不断调整，加大了算法的使用难度．高维问题在实际问题中，由于转化为个体的向量维数非常高，因此算法对高维问题的处理，将是很重要的。只有很好的处理高维问题，算法才能很好的应用于实际问题。

3. 高维问题     

	GA 对高维问题收敛速度很慢甚至很难收敛，但是 PSO 和 DE 则能很好解决。尤其是DE 算法，收敛速度很快而且结果很精确。

4. 收敛性能      

	对于优化问题，相对 GA，DE 和 PSO 算法收敛速度比较快，但是 PSO 容易陷入局部最优解，而且算法不稳定。

5. 应用广泛性       

	由于 GA 算法发明比较早，因此应用领域比较广泛，PSO 算法自从发明以来，已成为研究热点问题，这方面应用也比较多，而 DE 算法近几年才引起人们的关注而且算法性能好，因此应用领域将会增多。

## 偏微分
### 一元微分
在一元函数中的微分就是函数的切线。

<div align=center><img width="40%" src="Pics/math/94.png"/></div>

### 多元微分
而当我们进入三维空间后：

<div align=center><img width="40%" src="Pics/math/92.png"/></div>

我们就把这个切线称为f(x,y)对于x的偏微分。

	为什么是对于x的呢？
	因为这是y=0与f(x,y)的交线，在这条线上无论点怎么变化，都要满足y=0，即y是常数不会变化。

举一反三，所有y=C（C为常数）的平面与f(x,y)的交线都满足刚才说的特点：

<div align=center><img width="40%" src="Pics/math/93.png"/></div>

这些交线上的点的切线都是f(x,y)关于x的偏微分。
当然，如果f(x,y)与x=C（C为常数）得到的交线，这些交线的切线就是f(x,y)关于y的偏微分。
### 总结
偏微分就是：
- 固定y，变换x得到的就是f(x,y)关于x的偏微分
- 固定x，变换y得到的就是f(x,y)关于y的偏微分

## 偏导数
偏导数就是偏微分的斜率：

!> 注意：在三维空间中角度可以有不同的定义，计算斜率的时候我们是看下面这个α角

<div align=center><img width="40%" src="Pics/math/95.png"/></div>

## 全微分
其实，不光是y=C或者x=C这样的平面可以和f(x,y)相交得到交线，所有和xy平面垂直的平面都相交得到交线，这些交线都会有切线（微分）：

<table><tr>
	<td>
		<center><img width="70%" src="Pics/math/96.png"/></center>
	</td>
	<td>
		<center><img width="80%" src="Pics/math/97.png"/></center>
	</td>
</tr></table>

### 反例
如果这些切线都存在，并且这些切线（无数条）还都在同一个平面上（平面不是曲面），那么得到的这个平面就是全微分（也叫做切平面，或者说切空间）。

根据全微分的定义，如果全微分存在，那么偏导数、偏微分一定存在。
但是反过来不一定成立，即偏导数、偏微分存在，全微分不一定存在。因为偏导、偏微分只是x或者y方向的导数、微分，而全微分要求的是<strong>360°无死角</strong>。

举个例子：
<center><img width="20%" src="Pics/math/103.png"/></center>

我们考察这个函数在A=(0,0,0)点的全微分和偏微分的情况：

<table><tr>
	<td>
		<center><img width="70%" src="Pics/math/98.png"/></center>
		<center>f(x,y)</center>
	</td>
	<td>
		<center><img width="70%" src="Pics/math/99.png"/></center>
		<center>f(x,y)与y=0的交线</center>
	</td>
</tr></table>

在A=(0,0,0)点的微分（切线）很明显，就是交线自身(上图黑色粗线)，因此关于x的偏微分存在。

但是f(x,y)与y=x的交线在A=(0,0,0)点形成了一个尖点，很显然此时的微分不存在：

<table><tr>
	<td>
		<center><img width="70%" src="Pics/math/101.png"/></center>
	</td>
	<td>
		<center><img width="70%" src="Pics/math/102.png"/></center>
	</td>
</tr></table>

因此，全微分不存在。

### 总结
全微分就是：
- 360°微分都存在
- 并且这些微分要共面，得到的就是全微分

## 散度
### 通量
要理解散度，先要理解通量。通量简单来说，就是单位时间内通过的某个曲面的量。

### 太阳辐射与通量

我们都知道，人类离不开太阳。因为每时每刻我们都在接收太阳带给我们的能量。

<strong>那太阳每秒钟到底会向外辐射多少能量呢？</strong>

- 一种比较直观的办法，就是计算到底有多少能量通过太阳的表面。

沿着太阳表面，作一条封闭曲线（其实是封闭的曲面，因为太阳实际上是一个球体）：

<table><tr>
	<td>
		<center><img width="90%" src="Pics/math/104.png"/></center>
	</td>
	<td>
		<center><img width="70%" src="Pics/math/106.png"/></center>
	</td>
</tr></table>

图中绿色箭头表示能量的大小和方向，我们用<strong>A</strong>表示

粗略来说，我们把曲面上的<strong>A</strong>给加起来就是通过此曲面的通量。

但是这里有个细节问题，<strong>A</strong>在曲面上的不同的点的方向是不一样的，我们应该怎么相加？

### 通量的计算
可以观察到，能量在和表面垂直的时候取到最大值，相切的时候取到最小值。若它们不是直接垂直的，那么这时通过的能量的大小是<strong>A</strong>在与表面垂直方向的投影。

所以我们只需要关注<strong>A</strong>垂直于曲面的分量就可以了：

<center><img width="40%" src="Pics/math/107.png"/></center>

综上所述，通量就是把曲面上的<strong>A</strong>·<strong>n</strong>通过积分积起来。
我们很容易推出，对于曲面Σ，它的通量为：

<center><img width="10%" src="Pics/math/108.png"/></center>

### 回到散度
实际上还有一种计算太阳表面辐射的办法，只是这个办法有点局限性，如果我们计算的表面不封闭的话就不能用，比如下面这样只计算一半的曲面的通量的话就不能使用：

<center><img width="30%" src="Pics/math/109.png"/></center>

粗略地说，因为我们要计算整个太阳表面的辐射，每个点核聚变产生的辐射最终都会穿过太阳表面，因此我们把每个点的辐射加起来就可以得到太阳的表面辐射，即通量了。

为了通过这个思想来计算通量，我们就需要知道每个点的辐射强度（其实就是高斯公式），那么如何计算每一点的辐射强度呢？

根据微积分的基本思想，把将之前的封闭曲面缩小到极限为0，即几乎和辐射点重合时，用此时的通量，除以封闭曲面所围体积，就能得到此点的强度：

<center><img width="30%" src="Pics/math/110.png"/></center>

而此点的辐射强度就是散度。假设要求在向量场<strong>A</strong>中M点的散度：

<center><img width="30%" src="Pics/math/111.png"/></center>

其中，Omega 为封闭曲面Σ围成的区域，V为 Omega 的体积。

散度也有正负，太阳中，有些点并不产生核聚变（有可能此点是真空），辐射只是经过此点，此处的散度为0；而对于黑洞，它吸收能量，所以散度为负。

## 旋度
### 环流量
环流量简单来说，就是单位时间内环绕的某个曲线的量。

> 下面描述的都是在二维向量场中的情况，三维向量场中的情况类似，但是要更复杂一些。

比如，这是一汪湖水，其中箭头所指方向为水流方向，长短为水流的速度大小：

<center><img width="50%" src="Pics/math/112.png"/></center>

要计算一艘船在水流中受到多少旋转的力，就把这艘船丢到水里去。船的轮廓曲线抽象为封闭曲线，我们记为γ。单位时间内，这艘船在水场中受到旋转的力就称为<strong>环流量</strong>。

对于一个圆，我们可以比较直观的感受到：

<table><tr>
	<td>
		<center><img width="70%" src="Pics/math/113.png"/></center>
		<center>垂直力</center>
	</td>
	<td>
		<center><img width="70%" src="Pics/math/114.png"/></center>
		<center>和通量类似的，我们只需要切线方向的力</center>
	</td>
</tr></table>

因此整个环流量的表达式为：
<center><img width="10%" src="Pics/math/115.png"/></center>

### 回到旋度
类似于通量，我们也可以把各个点环流量的强度加起来，得到环流量。
而通过不断缩小封闭区域就可以得到环流量的强度，即旋度：

<center><img width="50%" src="Pics/math/116.png"/></center>

直观的，不严谨的，我们可以把旋度看成一个小漩涡。
我们也很容易推出旋度的表达式，M点的旋度表达式为:

<center><img width="20%" src="Pics/math/117.png"/></center>

其中，Σ为封闭曲线γ围成的区域，S为Σ的面积。
当然，旋度还有方向，下面再解释一下方向。

### 旋度的方向
遵循右手定则：
<center><img width="20%" src="Pics/math/right.jpg"/></center>

大拇指所指方向为旋度的方向，知道大拇指的方向就知道封闭曲线是顺时针还是逆时针旋转了。

<details>
<summary>例：</summary>
烟雾成顺时针或逆时针方向运动，对应的旋度在飞机前行的方向上：
<center><img width="60%" src="Pics/math/rot.jpg"/></center>
</details>


## 总结
### 方向导数与梯度的关系
- 方向导数是各个方向上的导数
- 偏导数连续才有梯度存在
- 梯度的方向是方向导数中取到最大值的方向，梯度的值是方向导数的最大值
- 在一元函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率
- 在多元函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点值变化最快的方向

### 全微分、偏导数、偏微分的关系
- 全微分存在偏导数、偏微分一定存在
- 偏导数、偏微分存在全微分不一定存在

### 通量、散度、环流量、旋度的关系
- 通量是单位时间内通过的某个曲面的量
- 散度是通量强度
- 环流量是单位时间内环绕的某个曲线的量
- 旋度是环流量强度

### 关于最优化方法
最优化方法的应用十分广泛，学习和工作中遇到的大多问题都可以建模成一种最优化模型进行求解，比如我们现在学习的机器学习算法，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。